<p>
    &nbsp;주어진 문제를 컴퓨터로 해결할 때에는 다음 순서를 따른다.
</p>
<ol>
    <li>문제의 수식화</li>
    <li>계산 방법 선택</li>
    <li>프로그램 구현</li>
    <li>프로그램 실행</li>
    <li>결과 해석</li>
</ol>
<p>
    이는 신경망을 구현할 때에도 당연히 적용된다. 여기서는 위 5가지 순서가 신경망을 구현할 때 어떻게 적용되는지 살펴볼 것이다.
</p>
<h2>
    문제의 수식화<sup>Modeling</sup>
</h2>
<p>
    &nbsp;미분 방정식을 공학에서 응용할 수 있는 대표적인 예는 스프링에 매달린 공의 운동을 기술할 때 나타난다. 자유감쇠진동을 기술하는
    미분방정식은 아래와 같다.
</p>
<div lang="latex">
    m \ddot{y} + c \dot{y} + ky = r(t)
</div>
<p>
    이와 같이 주어진 문제를 수식으로 표현하면 수식의 해를 찾을 때 이미 과학자/수학자들이 내놓은 풀이 방법을 이용할 수 있다.
    딥러닝(신경망)을 구성하는 행위도 이렇게 모델링된 문제의 최적해를 찾기 위한 한 방법이라 볼 수 있다. 물론 신경망 구현 자체가 이 단계에
    포함되지는 않지만, 문제를 수식으로 어떻게 표현하느냐에 따라 딥러닝을 활용하여 풀 수 있는지, 또는 어떤 신경망을 구성해야 할지가 판가름난다.
    모델링과 신경망 사이의 관계는 이후에 본격적으로 신경망에 대해 설명할 때 자연스럽게 언급될 것이다.
</p>
<h2>
    계산 방법 선택
</h2>
<p>
    &nbsp;모델링을 완료했다면, 다음으로 해야 할 일은 수식을 어떻게 계산할 것인지를 결정하는 일이다. 수학, 컴퓨터 과학에서는
    계산 방법을 단계적인 수행의 집합으로 만들어 놓은 것을 알고리즘<sup>Algorithm</sup>이라 한다.
    컴퓨터를 이용한 계산 속도가 비약적으로 증가하는 요즘 추세에 맞게 공학계에서 필요한 지긋지긋한 연산들을 효율적으로 계산할 수 있는
    알고리즘이 많이 고안되어 있다.(위에서 언급한 미분방정식은 Runge-Kutta-Nyström법으로 풀 수 있다)
    딥러닝 역시 어떤 문제를 해결하기 위한 알고리즘의 하나이고, 그 안에서도 오차역전파법, 드롭아웃 등의 세부 알고리즘으로 이루어져 있다.
</p>
<p>
    &nbsp;알고리즘은 수식을 계산하는 방법이기 때문에 알고리즘을 이해하려면 수식 자체에 대한 이해가 뒤따라줘야 한다.
    만약 딥러닝의 원리를 이해하고자 한다면 이를 기술할 때 등장하는 조금 무서워 보이는 수식을 이해해야 한다.
    때때로 이해하기 벅찬 내용이 나올 수 있지만, 한번 이해할 수만 있다면 알고리즘의 구현 자체는 어렵지 않은 경우가
    대부분이다. 딥러닝이라면 잘 모르겠지만..
</p>
<h2>
    프로그램 구현
</h2>
<p class="tip">
    이 부분은 필자가 딥러닝을 공부할 때 가장 고민했던 부분 중 하나이다. 이 문단은
    그런 필자의 고민을 한 곳에 쏟아부은 글이니, 글이 좀 길다 싶으면 밑에 요약만 보고 넘어가도 좋다.
</p>
<p>
    &nbsp;사람의 언어로 쓰여진 알고리즘은 컴퓨터가 수행할 수 없으므로 컴퓨터가 알아들을 수 있게 해줘야 한다.
    이 때는 어떤 프로그래밍 언어로 쓸지, 함수/클래스로 구현할지, 최적화는 어떤방식으로 할지를 결정해야 한다.
    나머지는 나중에 설명해도 될 듯 하니 언어에 대해서만 잠시 짚고 넘어가자.
</p>
<p>
    &nbsp;프로그래머들이 주로 사용하는 언어를 언급할 때는 C++과 Python을 빼놓을 수 없다. 이 두 언어만 놓고 봤을 때
    딥러닝의 구현에서 적합한 언어는 무엇일까? 속도면에서 볼때 C++는 Python에 비해 빠르다고 알려져있다.
    아마도 일반적으로는 그렇다. 애초에 Python의 표준 구현체인
    CPython이 C언어 위에서 만들어져 있기에 레이어가 하나 추가되어 추상화된 Python은 느릴 수 밖에 없다.(PyPy라는
    괴상망측한 Python 구현체는 때때로 C언어보다 빠르다고 하지만 일단 제쳐두자) 반면에 Python은 C++에 비해
    문법이 무진장 편하다. 컴파일 언어인 C++는 메모리 관리고 뭐고 신경 쓸게 많다. Python은 인터프리터 언어이면서도
    지원하는 편리한 모듈이 너무나 많아서 어떤 경우에는 C언어로 2년동안 한 일보다 Python으로 한달동안 한 일이 더 많다는 이야기도
    들려온다.
</p>
<p>
    &nbsp;현재는 컴퓨터 속도가 굉장히 빨라서 간단한 프로그램을 돌린다면 C++과 Python 간의 수행 시간 차이를 사람은 느끼지 못한다.
    따라서 그때는 Python을 쓰는 것이 더 이득이다. 딥러닝을 포함한 머신러닝 분야에서는 방대한 수치 계산이 동반되기 때문에
    속도가 필수 요소라면 C++를 원래는 써야 한다. 다만 한가지 꼼수가 존재하는데, Python에서 빠른 성능을 요구할 경우 C/C++을
    잠시 끌어다 쓸 수 있다. 한 예로 Python 상에서 행렬 계산을 수행할 때 주로 사용하는 라이브러리인 numpy는 주된 연산 처리를 C/C++로
    구현하여 성능을 해치지 않고 Python을 사용할 수 있게 했다. 여기까지 봤을 때는 아무리 봐도 Python이 굳이 딥러닝이 아니더라도
    거의 모든 프로그램의 구현에 더 좋아보인다. 실제로 요새 나오는 프레임워크들을 보면 Python이 머신러닝과 데이터 과학 분야에서
    적합한 언어로 사람들이 생각하고 있는 듯 하다. 사실 나도 대부분 그렇다고 생각한다.
    신경망을 작동시킬때 필수적인 행렬 연산을 빠른 속도로 처리하기 위해
    GPU의 연산칩을 활용한 GPGPU, 거기에 멀티스레딩 기술이 동원될 때가 많은데, 그런 쪽이 아무래도 저수준 처리에 속하고
    C++가 Python보다 저수준 언어일 뿐더러 필자가 C++에 애착이 좀 많이 있는지라
    어떻게든 Python을 깔려고 "Python은 GPGPU나 멀티스레딩 할 수 있냐" 하면서 찾아봤더니 할 수 있다고 나온다.
</p>
<p>
    &nbsp;이렇게 C++의 입지는 없어지는 건가 하고 필자는 잠시 실망에 잠긴 때가 있었다. 그런데 속도와 생산성 측면이 아닌
    전혀 다른 측면에서 실낱같은 희망을 볼 수 있었다. 현재 딥러닝을 배우고자 하는 사람의 사실상 25% 이상이 알파고를 보고 결심했을 것이다.
    이 알파고는 TPU라는 행렬 계산에 특화된 하드웨어를 사용했다. 동일한 전력이 들어왔을때 TPU는 GPU에 비해 월등한 계산력을 보여준다는
    것을 이미 말만 들어도 체감할 것이다. 딥러닝을 배우려는 사람 중에는 이런 맞춤형 하드웨어 역시 직접 설계하려는 괴물도 있을 것이고,
    아니더라도 누군가가 만든 딥러닝 전용 ASIC을 직접 활용하려는 사람도 있을 것이다. 이럴 때는 C++를 쓰는 것이 옳지 않겠는가?
</p>
<p>
    &nbsp;그럼 나에게 그런 쪽의 저수준 처리를 다룰 수 있는 능력이 있나 하면 그건 또 아니다. 그래도 이 강좌에서 나는
    C++을 중점적으로 다룰 것이다. 어차피 Python으로 딥러닝 가르치는 책은 많다. 이 강좌가 C++에 대한 심심한 위로가 되길.
</p>
<p class="conclusion">
    요약 : 되도록이면 Python을 사용하는게 이득이다. Python이 싫거나 쓸 상황이 안된다면 C++로 구현하라. 그리고 나는
    C++를 좋아하므로 C++로 코드를 작성할 것이다.
</p>
<h2>
    프로그램 실행
</h2>
<p>
    &nbsp;하드웨어도 괜찮게 뽑고, 신경망 구현도 좀 멋있게 해놓았다고 하자. 이제 이걸 어떻게 실행시킬 것인가?
    꽤 단순한 문제같기도 하다. C++이라면 그냥 실행 파일을 누른다던가, Python이라면 명령어로 "python deeplearning.py"
    이런식으로 하면 끝 아닌가? 하지만 여기서 이야기하는 건 실행 자체가 아닌 실행 파일 안에 신경망을 어떻게 내재시키고,
    입력을 어떻게 넣을 것인지, 무엇부터 실행해야 하는지에 대한 내용이다.
</p>
<p>
    &nbsp;신경망을 일련의 함수로 표현하든 객체로 표현하든 (심지어 프로세스로 표현하든), 그 신경망의 입력이나 출력값을 어떤 식으로 얻어올지,
    지도학습일 경우 프로그램에 정답 레이블은 어떻게 넣어줄지,
    어느 정도의 강도로 학습시킬 것인지, 기존의 신경망 데이터는 어떻게 불러올지 정도는 모두 기본적으로 고려되어야 한다.
    특히 당신이 최근에 닐 포드의 Functional Thinking이란 책을 보고 감명을 받았다던가 한다면(= 함수형 프로그래밍을 원한다면)
    저런 골칫거리들을 구현하기란 어려울 것이다.
</p>
<p>
    &nbsp;실행하다가 어딘가 잘못되면 어떻게 디버깅 할것인가? 단순한 소스코드상의 실수로 런타임 에러가 난다면 봐줄만 하다.
    필자가 경험하기로 신경망 데이터가 가끔/자주 폭주할때가 있다. 이게 무슨 뜻인지는 고사하고,
    100만개가 넘어가는 숫자들의 경향을 사람이 일일이 관찰하는 참사는 누구라도 막고 싶을 것이다.
    따라서 적어도 신경망을 구성하고 실행할 때에는, XOR 계산이나 가위바위보 정도의 간단한 문제부터 학습시켜보고 손글씨 인식같이
    좀 더 수준 높고 어려운 문제를 나중에 고려하여 실행시키는 것이 좋다.
</p>
<h2>
    결과 해석
</h2>
<p>
    &nbsp;수치해석학에서 컴퓨터로 계산하는 일은 보통 실수 연산으로 가정한다. 그리고 컴퓨터는 완벽한 실수의 값을 메모리에 저장하지 못한다.
    그러므로 컴퓨터로 문제를 푸는 일은 보통 오차를 고려해야 한다. 미분과 관련된 문제는 특히 조심해야 하지만, 
    딥러닝에서 신경망이 내놓는 출력값 자체는
    그렇게 정확하지 않아도 본질적인 답과 동일하다면 그것으로 충분하다. 따라서 현재 나오고 있는 딥러닝 함수 내장 프로세서는 보통 컴퓨터에서 쓰는
    대단히 높은 정밀도의 부동소수점 연산과 별개로 낮은 정밀도의 실수 연산도 지원하고 있다. 정밀도가 낮을 수록 오차가 쌓이긴 쉽지만
    계산 속도는 빠르고, 신경망의 정확도에 미치는 영향은 거의 없다는 것이 실험적으로 드러나고 있기 때문이다.
</p>
<p>
    &nbsp;신경망이 내놓는 출력의 형태는 문제의 유형에 따라 달라진다. 딥러닝을 위한 문제의 수식화는 회귀, 이진 분류,
    다클래스 분류 이 3가지로 구분하곤 한다. 회귀라는 말이 익숙치 않다면, 간단히 어떤 연속함수의 출력을 잘 찾아내는 일을 뜻한다고
    보면 된다. 구체적인 부분은 이후에 설명할 것이다.
</p>
